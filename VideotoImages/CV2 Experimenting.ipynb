{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Project is to experiment with python CV2 library.  goal is to be able to grab a live video stream with goal of eventually pairing it with the lifxtiles library to portray video on LIX tiles\n",
    "\n",
    "\n",
    "\n",
    "Many examples from here\n",
    "\n",
    "https://people.revoledu.com/kardi/tutorial/Python/Video+Analysis+using+OpenCV-Python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-contrib-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/29/0e9583011e535c23b92a0ec64ff449b9e05d86423a6e5a16385f569b04d6/opencv_contrib_python-4.2.0.34-cp38-cp38-macosx_10_9_x86_64.whl (60.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 60.3MB 507kB/s ta 0:00:011  8% |██▉                             | 5.3MB 5.7MB/s eta 0:00:10\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/christopheryoung/PycharmProjects/LIFXTilesPrivate/venv/lib/python3.8/site-packages (from opencv-contrib-python) (1.18.3)\n",
      "Installing collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.2.0.34\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Could not read the image.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Could not read the image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopheryoung/PycharmProjects/LIFXTilesPrivate/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2 as cv\n",
    "import sys\n",
    "img = cv.imread('../images/Maika.jpg')\n",
    "if img is None:\n",
    "    sys.exit(\"Could not read the image.\")\n",
    "cv.imshow(\"Display window\", img)\n",
    "k = cv.waitKey(0)\n",
    "if k == ord(\"s\"):\n",
    "    cv.imwrite(\"starry_night.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function imread:\n",
      "\n",
      "imread(...)\n",
      "    imread(filename[, flags]) -> retval\n",
      "    .   @brief Loads an image from a file.\n",
      "    .   \n",
      "    .   @anchor imread\n",
      "    .   \n",
      "    .   The function imread loads an image from the specified file and returns it. If the image cannot be\n",
      "    .   read (because of missing file, improper permissions, unsupported or invalid format), the function\n",
      "    .   returns an empty matrix ( Mat::data==NULL ).\n",
      "    .   \n",
      "    .   Currently, the following file formats are supported:\n",
      "    .   \n",
      "    .   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n",
      "    .   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n",
      "    .   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n",
      "    .   -   Portable Network Graphics - \\*.png (see the *Note* section)\n",
      "    .   -   WebP - \\*.webp (see the *Note* section)\n",
      "    .   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n",
      "    .   -   PFM files - \\*.pfm (see the *Note* section)\n",
      "    .   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n",
      "    .   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n",
      "    .   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n",
      "    .   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n",
      "    .   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n",
      "    .   \n",
      "    .   @note\n",
      "    .   -   The function determines the type of an image by the content, not by the file extension.\n",
      "    .   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n",
      "    .   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n",
      "    .       Results may differ to the output of cvtColor()\n",
      "    .   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n",
      "    .       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n",
      "    .       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n",
      "    .       that currently these native image loaders give images with different pixel values because of\n",
      "    .       the color management embedded into MacOSX.\n",
      "    .   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n",
      "    .       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n",
      "    .       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n",
      "    .       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n",
      "    .   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n",
      "    .       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n",
      "    .       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n",
      "    .       [Vector](http://www.gdal.org/ogr_formats.html).\n",
      "    .   -   If EXIF information are embedded in the image file, the EXIF orientation will be taken into account\n",
      "    .       and thus the image will be rotated accordingly except if the flag @ref IMREAD_IGNORE_ORIENTATION is passed.\n",
      "    .   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n",
      "    .   -   By default number of pixels must be less than 2^30. Limit can be set using system\n",
      "    .       variable OPENCV_IO_MAX_IMAGE_PIXELS\n",
      "    .   \n",
      "    .   @param filename Name of file to be loaded.\n",
      "    .   @param flags Flag that can take values of cv::ImreadModes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv.imread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"IMG_2128.MOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e726ce5504ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/highgui/src/window.cpp:376: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3c56e1a3f337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'win'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/highgui/src/window.cpp:376: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('win', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function imshow:\n",
      "\n",
      "imshow(...)\n",
      "    imshow(winname, mat) -> None\n",
      "    .   @brief Displays an image in the specified window.\n",
      "    .   \n",
      "    .   The function imshow displays an image in the specified window. If the window was created with the\n",
      "    .   cv::WINDOW_AUTOSIZE flag, the image is shown with its original size, however it is still limited by the screen resolution.\n",
      "    .   Otherwise, the image is scaled to fit the window. The function may scale the image, depending on its depth:\n",
      "    .   \n",
      "    .   -   If the image is 8-bit unsigned, it is displayed as is.\n",
      "    .   -   If the image is 16-bit unsigned or 32-bit integer, the pixels are divided by 256. That is, the\n",
      "    .       value range [0,255\\*256] is mapped to [0,255].\n",
      "    .   -   If the image is 32-bit or 64-bit floating-point, the pixel values are multiplied by 255. That is, the\n",
      "    .       value range [0,1] is mapped to [0,255].\n",
      "    .   \n",
      "    .   If window was created with OpenGL support, cv::imshow also support ogl::Buffer , ogl::Texture2D and\n",
      "    .   cuda::GpuMat as input.\n",
      "    .   \n",
      "    .   If the window was not created before this function, it is assumed creating a window with cv::WINDOW_AUTOSIZE.\n",
      "    .   \n",
      "    .   If you need to show an image that is bigger than the screen resolution, you will need to call namedWindow(\"\", WINDOW_NORMAL) before the imshow.\n",
      "    .   \n",
      "    .   @note This function should be followed by cv::waitKey function which displays the image for specified\n",
      "    .   milliseconds. Otherwise, it won't display the image. For example, **waitKey(0)** will display the window\n",
      "    .   infinitely until any keypress (it is suitable for image display). **waitKey(25)** will display a frame\n",
      "    .   for 25 ms, after which display will be automatically closed. (If you put it in a loop to read\n",
      "    .   videos, it will display the video frame-by-frame)\n",
      "    .   \n",
      "    .   @note\n",
      "    .   \n",
      "    .   [__Windows Backend Only__] Pressing Ctrl+C will copy the image to the clipboard.\n",
      "    .   \n",
      "    .   [__Windows Backend Only__] Pressing Ctrl+S will show a dialog to save the image.\n",
      "    .   \n",
      "    .   @param winname Name of the window.\n",
      "    .   @param mat Image to be shown.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.imshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Video Capture\n",
    "The following code is to capture the video using the webcam on your laptop. It will open a window contains the images captured by the webcam. There is no processing aside from simple showing it. Press q to quit the window.\n",
    "Tips: If you found error during your experiments, restart the kernel and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()  # ret = 1 if the video is captured; frame is the image\n",
    "    \n",
    "    # Our operations on the frame come here    \n",
    "    img = cv2.flip(frame,1)   # flip left-right  \n",
    "    img = cv2.flip(img,0)     # flip up-down\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video Capture',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Video\n",
    "The following code come from OpenCV documentation. I just change the codec to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# create writer object\n",
    "fileName='output.mpg'  # change the file name if needed\n",
    "imgSize=(640,480)\n",
    "frame_per_second=30.0\n",
    "writer = cv2.VideoWriter(fileName, cv2.VideoWriter_fourcc(*\"MJPG\"), frame_per_second,imgSize)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        writer.write(frame)                   # save the frame into video file\n",
    "        cv2.imshow('Video Capture',frame)     # show on the screen\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): # press q to quit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Playing Video\n",
    "Now let us load and play the video that we just captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "fileName='output.mpg'  # change the file name if needed\n",
    "\n",
    "cap = cv2.VideoCapture(fileName)          # load the video\n",
    "while(cap.isOpened()):                    # play the video by reading frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        # optional: do some image processing here \n",
    "    \n",
    "        cv2.imshow('frame',frame)              # show the video\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Window Size\n",
    "You can change the size of the window by setting the frame size (e.g. 640x480; 320x240; 960x720), larger is slower ad you can see the delay from your movement.\n",
    "ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH,320) ret = cap.set(cv2.CAP_PROP_FRAME_HEIGHT,240)\n",
    "Another faster way is to use cv2.resize() function:\n",
    "frame=cv2.resize(frame,None,fx=scaling_factor,fy=scaling_factor,interpolation=cv2.INTER_AREA)\n",
    "Experiment:\n",
    "Try to change the window size using capture setting or resize function\n",
    "Try to change the scaling factor of fx and fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "scaling_factorx=0.5\n",
    "scaling_factory=0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()  # ret = 1 if the video is captured; frame is the image\n",
    "    \n",
    "    # set frame size (e.g. 640x480; 320x240; 960x720), larger is slower    \n",
    "    #ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH,320)\n",
    "    #ret = cap.set(cv2.CAP_PROP_FRAME_HEIGHT,240)\n",
    "    frame=cv2.resize(frame,None,fx=scaling_factorx,fy=scaling_factory,interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Our operations on the frame come here    \n",
    "    img = frame\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Smaller Window',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Transformation\n",
    "I use basic video capture code. Now we have a simple color transformation\n",
    "Experiment: Try the following color transformation:\n",
    "+ img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # BGR color to RGB\n",
    "+ img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # RGB color to BGR\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  # BGR color to gray level\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)  # RGB color to gray level\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)   # BGR color to HSV\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_RGB2HSV)   # RGB color to HSV\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_RGB2HLS)   # RGB color to HLS\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_BGR2HLS)   # BGR color to HLS\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_BGR2XYZ)   # RGB color to CIE XYZ.Rec 709\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_RGB2XYZ)   # RGB color to CIE XYZ.Rec 709\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_BGR2Lab)   # BGR color to CIE L\\*a\\*b\\*\n",
    "+ img = cv2.cvtColor(frame,cv2.COLOR_RGB2Luv)   # RGB color to CIE L\\*u\\*v\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d3655c3765ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Capture frame-by-frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Our operations on the frame come here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()        \n",
    "    \n",
    "    # Our operations on the frame come here    \n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    #img = cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)  # BGR color to gray level\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Gray',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Enhancement\n",
    "Using Histogram equalization, we can enhance the contrast of the image. OpenCV equalizeHist() function is working only for grayscale image. The following code is useful to enhance the constrast of color image.\n",
    "check OpenCV documentation on histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def equalizeHistColor(frame):\n",
    "    # equalize the histogram of color image\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)  # convert to HSV\n",
    "    img[:,:,2] = cv2.equalizeHist(img[:,:,2])     # equalize the histogram of the V channel\n",
    "    return cv2.cvtColor(img, cv2.COLOR_HSV2RGB)   # convert the HSV image back to RGB format\n",
    "\n",
    "\n",
    "# start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()        \n",
    "    \n",
    "    # Our operations on the frame come here    \n",
    "    #img = frame\n",
    "    img = equalizeHistColor(frame)\n",
    "    \n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Histogram Equalization',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Transformation\n",
    "Let us have some fun to warp our video capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def WarpImage(frame):\n",
    "    ax,bx=10.0,100\n",
    "    ay,by=20.0,120\n",
    "    img=np.zeros(frame.shape,dtype=frame.dtype)\n",
    "    rows,cols=img.shape[:2]\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            offset_x=int(ax*math.sin(2*math.pi*i/bx))\n",
    "            offset_y=int(ay*math.cos(2*math.pi*j/by))\n",
    "            if i+offset_y<rows and j+offset_x<cols:\n",
    "                img[i,j]=frame[(i+offset_y)%rows,(j+offset_x)%cols]\n",
    "            else:\n",
    "                img[i,j]=0\n",
    "    return img\n",
    "\n",
    "def equalizeHistColor(frame):\n",
    "    # equalize the histogram of color image\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)  # convert to HSV\n",
    "    img[:,:,2] = cv2.equalizeHist(img[:,:,2])     # equalize the histogram of the V channel\n",
    "    return cv2.cvtColor(img, cv2.COLOR_HSV2RGB)   # convert the HSV image back to RGB format\n",
    "\n",
    "\n",
    "# start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read() \n",
    "    frame=cv2.resize(frame,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Our operations on the frame come here \n",
    "    if ret==1:\n",
    "        #img = WarpImage(frame)\n",
    "        img = equalizeHistColor(WarpImage(frame))\n",
    "    else:\n",
    "        img = equalizeHistColor(frame)\n",
    "        \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Warped',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection & Smoothing\n",
    "Reuse the Basic Video Capture code. Let us change our operation into an Edge Detection.\n",
    "First, we use Canny Edge detection with three parameters. Check OpenCV documentation about Canny().\n",
    "Try to improve the edge detection\n",
    "Experiments:\n",
    "Try to change the parameter values of Edge Detection\n",
    "Try to use or not to use Smoothing filter before or after the edge detection\n",
    "frame = cv2.GaussianBlur(frame, (kernelSize,kernelSize), 0, 0) # Gaussian Blur smoothing filter\n",
    "frame = cv2.medianBlur(frame, kernelSize) # Median Blur smoothing filter\n",
    "frame = cv2.blur(frame,(kernelSize,kernelSize)) # Average Blur smoothing filter\n",
    "frame = cv2.bilateralFilter(frame,9,75,75) # Bilateral Filter for smoothing filter\n",
    "Try to change kernel size of the smoothing filter\n",
    "Try to change the Edge Detection Algorithm:\n",
    "frame = cv2.Canny(frame,parameter1,parameter2,intApertureSize) # Canny edge detection\n",
    "frame = cv2.Laplacian(frame,cv2.CV_64F) # Laplacian edge detection\n",
    "frame = cv2.Sobel(frame,cv2.CV_64F,1,0,ksize=kernelSize) # X-direction Sobel edge detection\n",
    "frame = cv2.Sobel(frame,cv2.CV_64F,0,1,ksize=kernelSize) # Y-direction Sobel edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "kernelSize=21   # Kernel Bluring size \n",
    "\n",
    "# Edge Detection Parameter\n",
    "parameter1=20\n",
    "parameter2=60\n",
    "intApertureSize=1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()    \n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    frame = cv2.GaussianBlur(frame, (kernelSize,kernelSize), 0, 0)\n",
    "    frame = cv2.Canny(frame,parameter1,parameter2,intApertureSize)  # Canny edge detection\n",
    "    #frame = cv2.Laplacian(frame,cv2.CV_64F) # Laplacian edge detection\n",
    "    #frame = cv2.Sobel(frame,cv2.CV_64F,1,0,ksize=kernelSize) # X-direction Sobel edge detection\n",
    "    #frame = cv2.Sobel(frame,cv2.CV_64F,0,1,ksize=kernelSize) # Y-direction Sobel edge detection\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Canny',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Impose\n",
    "Let us use the edge detection as our mask and superimpose it with the original image. First we reverse the edge detection result using bitwise Not to inverse it. Then, we use bitwise And to superimpose with the blur image.\n",
    "You will see yourself with an edge.\n",
    "Experiments\n",
    "Use different smoothing algorithm and various parameters values.\n",
    "Use Bluring frame rather than the original image to be superimposed.\n",
    "Use edge detection directly without inverting it. What happen if you did not invert the edge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "kernelSize=21   # Kernel Bluring size \n",
    "\n",
    "# Edge Detection Parameter\n",
    "parameter1=10\n",
    "parameter2=40\n",
    "intApertureSize=1\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame1 = cap.read()    \n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    frame = cv2.GaussianBlur(frame1, (kernelSize,kernelSize), 0, 0) \n",
    "    edge = cv2.Canny(frame,parameter1,parameter2,intApertureSize)  # Canny edge detection\n",
    "    mask_edge = cv2.bitwise_not(edge)\n",
    "    frame = cv2.bitwise_and(frame1,frame1,mask = mask_edge)   \n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Super Impose',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "Use threshold to create a mask. A mask is a binary image.\n",
    "\n",
    "Adaptive threshold has the following parameters:\n",
    "\n",
    "src – Source 8-bit single-channel image.\n",
    "dst – Destination image of the same size and the same type as src .\n",
    "maxValue – Non-zero value assigned to the pixels for which the condition is satisfied. Put 255.\n",
    "adaptiveMethod – Adaptive thresholding algorithm to use, ADAPTIVE_THRESH_MEAN_C or ADAPTIVE_THRESH_GAUSSIAN_C .\n",
    "thresholdType – Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV .\n",
    "blockSize – Size of a pixel neighborhood that is used to calculate a threshold value for the pixel: 3, 5, 7, and so on.\n",
    "C – Constant subtracted from the mean or weighted mean.\n",
    "\n",
    "See also: documentation on AdaptiveThreshold\n",
    "\n",
    "Experiments:\n",
    "\n",
    "What happen if you do not use smoothing before the threshold?\n",
    "Change the threshold values between 0 to 255\n",
    "Change the thresholding methods:\n",
    "ret, mask = cv2.threshold(gray,threshold1, threshold2,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "mask = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,blockSize,constant)\n",
    "mask = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,blockSize,constant)\n",
    "ret, mask = cv2.threshold(gray, threshold1, threshold2, cv2.THRESH_BINARY)\n",
    "\n",
    "what is the effect if you invert the mask instead of merely the mask?\n",
    "\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "img = cv2.bitwise_and(frame,frame,mask = mask_inv)\n",
    "\n",
    "What happen if you see only the mask:\n",
    "img=mask\n",
    "\n",
    "What is you use gray image instead of the color frame?\n",
    "img = cv2.bitwise_and(gray,gray,mask = mask) img = cv2.bitwise_and(frame,frame,mask = mask)\n",
    "\n",
    "Why if you mixed gray image and color frame in bitwise And, it does not work?\n",
    "\n",
    "What happen if you add or remove image blendng:\n",
    "\n",
    "img = cv2.addWeighted(frame,0.1,img,0.9,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def equalizeHistColor(frame):\n",
    "    # equalize the histogram of color image\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)  # convert to HSV\n",
    "    img[:,:,2] = cv2.equalizeHist(img[:,:,2])     # equalize the histogram of the V channel\n",
    "    return cv2.cvtColor(img, cv2.COLOR_HSV2RGB)   # convert the HSV image back to RGB format\n",
    "\n",
    "\n",
    "threshold1=100\n",
    "threshold2=200\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()  # ret = 1 if the video is captured; frame is the image\n",
    "    \n",
    "    # equalize the histogram of color image\n",
    "    frame1 = equalizeHistColor(frame) \n",
    "    gray = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(21,21),0)\n",
    "\n",
    "    #ret, mask = cv2.threshold(blur, threshold1, threshold2, cv2.THRESH_BINARY)\n",
    "    ret, mask = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    #ret, mask = cv2.threshold(blur,threshold1, threshold2,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    #mask = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    #mask = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    kernel = np.ones((3, 3), np.uint8)  \n",
    "    mask=cv2.erode(mask,kernel,iterations=7) # morphology erosion   \n",
    "    mask=cv2.dilate(mask,kernel,iterations=5) # morphology dilation\n",
    "    \n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "    img = cv2.bitwise_and(frame1,frame1,mask = mask_inv)\n",
    "    img = cv2.addWeighted(frame1,0.1,img,0.9,0)\n",
    "    \n",
    "    #img=mask\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Thresholding-Otsu',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Countour\n",
    "\n",
    "Two main functions of contours are :\n",
    "\n",
    "img, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,chainRuleApproximation)\n",
    "img=cv2.drawContours(img, contours, index, colorTuple, thickness)\n",
    "\n",
    "The arguments are:\n",
    "\n",
    "img = image\n",
    "index=-1 means show all contours, 0-len(contours) means to show each contour\n",
    "chainRuleApproximation is either:\n",
    "cv2.CHAIN_APPROX_SIMPLE: to give only 4 points in a rectangle\n",
    "cv2.CHAIN_APPROX_NONE: to give all points\n",
    "colorTuple is any BGR color. For instance:\n",
    "(255,0,0) for Blue\n",
    "(0,255,0) for Green\n",
    "(0,0,255) for Red\n",
    "thickness is integer 1-10\n",
    "\n",
    "Check the documentation of FindContours , Contour Features, Draw Contour\n",
    "\n",
    "The code below capture video images via web cam, then show the image overlayed with the contour and bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e23cb5e62242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#blur = cv2.GaussianBlur(gray,(21,21),0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY_INV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_TREE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontourArea\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# find the largest contour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time \n",
    "\n",
    "color=(255,0,0)\n",
    "thickness=2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()  # ret = 1 if the video is captured; frame is the image\n",
    "    \n",
    "    # Our operations on the frame come here    \n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    #blur = cv2.GaussianBlur(gray,(21,21),0)\n",
    "    ret,thresh = cv2.threshold(gray,10,20,cv2.THRESH_BINARY_INV)\n",
    "    img1, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)   \n",
    "    if len(contours) != 0:\n",
    "            c = max(contours, key = cv2.contourArea) # find the largest contour\n",
    "            x,y,w,h = cv2.boundingRect(c)          # get bounding box of largest contour\n",
    "            #img2=cv2.drawContours(frame, c, -1, color, thickness) # draw largest contour\n",
    "            img2=cv2.drawContours(frame, contours, -1, color, thickness) # draw all contours\n",
    "            img3 = cv2.rectangle(img2,(x,y),(x+w,y+h),(0,0,255),2)  # draw red bounding box in img\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Contour',img3)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Optical Flow\n",
    "\n",
    "The following code is a slighly modified version of Lucas Kanade optical flow from OpenCV tutorial. It created aura near your head. As usual, press q to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[...,1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()    \n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "    prvs = next\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Optical Flow Aura',bgr)\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "    \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Difference: Motion Detection\n",
    "\n",
    "One way to detect a moving object is through image difference. Two images are captured with a slight time delay of 1/25 seconds.\n",
    "\n",
    "Once we have the image difference, get the contour out of it and put the bounding box out of the contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a4455c664fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# get contours and set bounding box from contours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mimg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETR_TREE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHAIN_APPROX_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time \n",
    "\n",
    "color=(255,0,0)\n",
    "thickness=2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture two frames\n",
    "    ret, frame1 = cap.read()  # first image\n",
    "    time.sleep(1/25)          # slight delay\n",
    "    ret, frame2 = cap.read()  # second image \n",
    "    img1 = cv2.absdiff(frame1,frame2)  # image difference\n",
    "    \n",
    "    # get theshold image\n",
    "    gray = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(21,21),0)\n",
    "    ret,thresh = cv2.threshold(blur,200,255,cv2.THRESH_OTSU)\n",
    "    \n",
    "    # combine frame and the image difference\n",
    "    img2 = cv2.addWeighted(frame1,0.9,img1,0.1,0)\n",
    "    \n",
    "    # get contours and set bounding box from contours\n",
    "    img3, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    if len(contours) != 0:\n",
    "        for c in contours:\n",
    "            rect = cv2.boundingRect(c)\n",
    "            height, width = img3.shape[:2]            \n",
    "            if rect[2] > 0.2*height and rect[2] < 0.7*height and rect[3] > 0.2*width and rect[3] < 0.7*width: \n",
    "                x,y,w,h = cv2.boundingRect(c)            # get bounding box of largest contour\n",
    "                img4=cv2.drawContours(img2, c, -1, color, thickness)\n",
    "                img5 = cv2.rectangle(img2,(x,y),(x+w,y+h),(0,0,255),2)  # draw red bounding box in img\n",
    "            else:\n",
    "                img5=img2\n",
    "    else:\n",
    "        img5=img2\n",
    "        \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Motion Detection by Image Difference',img2)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06d32fbe16e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_casc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m                     \u001b[0;31m# default if face is not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "folder='C:\\\\Users\\\\Teknomo\\\\Downloads\\\\opencv\\\\sources\\\\data\\\\haarcascades_cuda\\\\'\n",
    "face_casc = cv2.CascadeClassifier(folder+'haarcascade_frontalface_default.xml')\n",
    "eye_casc=cv2.CascadeClassifier(folder+'haarcascade_eye.xml')\n",
    "\n",
    "color=(0,255,0)\n",
    "thickness=3\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()  # ret = 1 if the video is captured; frame is the image\n",
    "    \n",
    "    # Our operations on the frame come here \n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    faces = face_casc.detectMultiScale(gray, scaleFactor=1.1,minNeighbors=3)\n",
    "    \n",
    "    img=frame                     # default if face is not found\n",
    "    for(x,y,w,h) in faces:\n",
    "        roi_gray=gray[y:y+h,x:x+w]\n",
    "        roi_color=frame[y:y+h,x:x+w]\n",
    "        #img=cv2.rectangle(frame, (x, y), (x + w, y + h), color, thickness) # box for face\n",
    "        eyes=eye_casc.detectMultiScale(roi_gray)\n",
    "        for(x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "            center=(int(x_eye+0.5*w_eye),int(y_eye+0.5*h_eye))\n",
    "            radius=int(0.3*(w_eye+h_eye))\n",
    "            img=cv2.circle(roi_color,center,radius,color,thickness)\n",
    "            #img=cv2.circle(frame,center,radius,color,thickness)\n",
    "\n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Face Detection Harr',img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction\n",
    "\n",
    "To generate the background image, take average of frames.The camera is assumed to be static. Foreground is equal to the current frame minus the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ebc503cd0009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbg_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbg_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#the above code is the same as:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfgmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbg_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# create foreground\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "alpha=0.999\n",
    "isFirstTime=True\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()  # ret = 1 if the video is captured; frame is the image\n",
    "    frame=cv2.resize(frame,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    #create background    \n",
    "    if isFirstTime==True:\n",
    "        bg_img=frame\n",
    "        isFirstTime=False\n",
    "    else:\n",
    "        bg_img = dst = cv2.addWeighted(frame,(1-alpha),bg_img,alpha,0)\n",
    "    #the above code is the same as:\n",
    "    fgmask = bg_img.apply(frame)\n",
    "    \n",
    "    # create foreground\n",
    "    #fg_img=cv2.subtract(frame,bg_img)\n",
    "    fg_img = cv2.absdiff(frame,bg_img)  \n",
    "    \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video Capture',frame)\n",
    "    cv2.imshow('Background',bg_img)\n",
    "    cv2.imshow('Foreground',fgmask)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Angry Birds Local Video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "fileName='IMG_2128.MOV'  # change the file name if needed\n",
    "\n",
    "cap = cv2.VideoCapture(fileName)          # load the video\n",
    "while(cap.isOpened()):                    # play the video by reading frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        # optional: do some image processing here \n",
    "    \n",
    "        cv2.imshow('frame',frame)              # show the video\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
